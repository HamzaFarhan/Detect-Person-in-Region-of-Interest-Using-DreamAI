{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Person in Region of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be using DreamAI to train a model. This is an example of what we can do with our general purpose object detection model that we trained here: https://github.com/HamzaFarhan/COCO-Object-Detection-using-DreamAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Make sure to change this path to your folder with DreamAI\n",
    "\n",
    "sys.path.insert(0, '/home/farhan/hamza/dreamai/') # Folder with DreamAI\n",
    "\n",
    "# Things below are all included in the dreamai folder\n",
    "\n",
    "import utils\n",
    "import obj_utils\n",
    "from dai_imports import*\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the object of the model that we trained\n",
    "\n",
    "net = data_processing.load_obj('best_obj_net.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to calculate the intersection over unio (IoU) of two rectangles\n",
    "# Source: ronny rest (can't seem to find the link to their website)\n",
    "\n",
    "def get_iou(a, b, epsilon=1e-5):\n",
    "    \"\"\" Given two boxes `a` and `b` defined as a list of four numbers:\n",
    "            [x1,y1,x2,y2]\n",
    "        where:\n",
    "            x1,y1 represent the upper left corner\n",
    "            x2,y2 represent the lower right corner\n",
    "        It returns the Intersect of Union score for these two boxes.\n",
    "\n",
    "    Args:\n",
    "        a:          (list of 4 numbers) [x1,y1,x2,y2]\n",
    "        b:          (list of 4 numbers) [x1,y1,x2,y2]\n",
    "        epsilon:    (float) Small value to prevent division by zero\n",
    "\n",
    "    Returns:\n",
    "        (float) The Intersect of Union score.\n",
    "    \"\"\"\n",
    "    # COORDINATES OF THE INTERSECTION BOX\n",
    "    x1 = max(a[0], b[0])\n",
    "    y1 = max(a[1], b[1])\n",
    "    x2 = min(a[2], b[2])\n",
    "    y2 = min(a[3], b[3])\n",
    "\n",
    "    # AREA OF OVERLAP - Area where the boxes intersect\n",
    "    width = (x2 - x1)\n",
    "    height = (y2 - y1)\n",
    "    # handle case where there is NO overlap\n",
    "    if (width<0) or (height <0):\n",
    "        return 0.0\n",
    "    area_overlap = width * height\n",
    "\n",
    "    # COMBINED AREA\n",
    "    area_a = (a[2] - a[0]) * (a[3] - a[1])\n",
    "    area_b = (b[2] - b[0]) * (b[3] - b[1])\n",
    "    area_combined = area_a + area_b - area_overlap\n",
    "\n",
    "    # RATIO OF AREA OF OVERLAP OVER COMBINED AREA\n",
    "    iou = area_overlap / (area_combined+epsilon)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dai_roi(video_path, person_net, detection_conf = 0.3, nms_overlap = 0.4, iou_thresh = 0.3,\n",
    "            frame_size = (256,256), output = 'roi_out.mp4'):\n",
    "\n",
    "    writer = None\n",
    "    vs = cv2.VideoCapture(video_path)\n",
    "    time.sleep(2.0)\n",
    "    frame_number = 0\n",
    "    selected = False\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Grab a frame from the video stream\n",
    "        (grabbed, frame) = vs.read()\n",
    "        if grabbed:\n",
    "            frame_number+=1\n",
    "            old_H,old_W = frame.shape[:2]\n",
    "        # If the frame was not grabbed, then we have reached the end of the video\n",
    "        if not grabbed:\n",
    "            break\n",
    "            \n",
    "# Our model was trained on imgages of size 'frame_size' and in object detection, we can't run our model on\n",
    "# images of a different size, so we will resize the frame just for predictions\n",
    "        new_frame = cv2.resize(frame,frame_size)\n",
    "        (H, W) = new_frame.shape[:2]\n",
    "        height_scale = old_H/H\n",
    "        width_scale = old_W/W\n",
    "        \n",
    "        # If RoI has been selected, draw it on the frame and then predict objects in the frame\n",
    "        \n",
    "        if selected:           \n",
    "            \n",
    "            # Draw RoI\n",
    "            \n",
    "            (x1, y1, x2, y2) = roi\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2),(0, 0, 255), 2)\n",
    "            cv2.rectangle(frame, (x1, y2 + 25), (x2, y2), (0, 0, 255), cv2.FILLED)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(frame, 'ROI', (x1 + 6, y2 + 19), font, 0.5, (255, 255, 255), 1)\n",
    "            \n",
    "            # Take the resized 'new_frame' and turn it into a PyTorch batch to be fed to the model\n",
    "            \n",
    "            img_batch = utils.get_test_input(imgs=[new_frame],size=frame_size)\n",
    "            \n",
    "            # Predict. This will output the bounding boxes and the labels of all objects detected\n",
    "            \n",
    "            object_net_pred = person_net.predict_objects(img_batch,score_thresh=detection_conf,\n",
    "                                                 nms_overlap=nms_overlap)\n",
    "            \n",
    "            # If any object has been found, further filter them to objects with the label 'person'\n",
    "            \n",
    "            objects_found = len(object_net_pred[0]) > 0\n",
    "            if objects_found:\n",
    "                person_locations = np.array(object_net_pred[0])[np.array(object_net_pred[1]) == 'person']\n",
    "                if len(person_locations) > 0:\n",
    "                    \n",
    "                    # Since our 'new_frame' was the resized frame,\n",
    "                    # we must scale the predicted bounding boxes back to the original frame size\n",
    "                    \n",
    "                    person_locations = [[int(np.ceil(f[0]*width_scale)),int(np.ceil(f[1]*height_scale)),\n",
    "                                       int(np.ceil(f[2]*width_scale)),int(np.ceil(f[3]*height_scale))] \n",
    "                                       for f in person_locations]\n",
    "                    \n",
    "                # Draw a bounding box around a person if their IoU with the RoI is greater than 'iou_thresh'\n",
    "                    \n",
    "                    for (x1,y1,x2,y2) in person_locations:\n",
    "\n",
    "                        iou = get_iou(roi,(x1,y1,x2,y2))\n",
    "                        if (iou > iou_thresh):\n",
    "                            cv2.rectangle(frame, (x1, y1), (x2, y2),(255, 0, 0), 2)\n",
    "                            cv2.rectangle(frame, (x1, y2 + 25), (x2, y2), (255, 0, 0), cv2.FILLED)\n",
    "                            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "                            cv2.putText(frame, 'person', (x1 + 6, y2 + 19), font, 0.5, (255, 255, 255), 1)\n",
    "                       \n",
    "        # Show the output frame\n",
    "        \n",
    "        cv2.imshow(\"ROI Video\", frame)\n",
    "        \n",
    "        # Write the frame to the output file\n",
    "        \n",
    "        if writer is None:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"DIVX\")\n",
    "            writer = cv2.VideoWriter(output, fourcc, 24,\n",
    "                (frame.shape[1], frame.shape[0]), True)\n",
    "        \n",
    "        writer.write(frame)\n",
    "        \n",
    "        # Slight delay if no RoI selected just to make it easier to select\n",
    "        \n",
    "        if not selected:\n",
    "            time.sleep(0.09)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # If 's' is pressed, video pauses and RoI can be selected. Press 'enter' or 'space' after selecting\n",
    "        \n",
    "        if key == ord(\"s\"):            \n",
    "            selected = True\n",
    "            roi = cv2.selectROI(\"ROI Video\", frame, fromCenter=False,\n",
    "                showCrosshair=True)\n",
    "            roi = roi[0],roi[1],roi[0]+roi[2],roi[1]+roi[3]\n",
    "            \n",
    "        # If 'w' is pressed, remove the current RoI    \n",
    "            \n",
    "        elif key == ord(\"w\"):\n",
    "            selected = False\n",
    "        \n",
    "        # If 'q' is pressed, quit\n",
    "            \n",
    "        elif key == ord(\"q\"):\n",
    "            break    \n",
    "            \n",
    "    writer.release()\n",
    "    vs.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'dai_roi' takes a video path and an object detection model and some additional paramaters.\n",
    "When run, it starts displaying the video in a window, there is a slight delay between frames so that it is easier to mark the region of interest (RoI).\n",
    "\n",
    "When the user presses 's', the video pauses and then they can draw a box on the RoI, then when they press 'space' or 'enter', the video resumes with the RoI now being displayed.\n",
    "\n",
    "Now whenever a person will enter the RoI, a bounding box will be displayed around them as long as they are in the RoI.\n",
    "\n",
    "Pressing 'w' will reset the RoI and until the user presses 's' again, there will be no RoI.\n",
    "\n",
    "Pressing 'q' will quit the video and the final video with the bounding boxes being displayed will be saved in the 'output' file.\n",
    "\n",
    "You can see the output generated video called 'rf_roi_github.mp4' as well as the screen recording of the demo called 'roi_usage_demo.mp4'\n",
    "\n",
    "If you want to try it yourself, just run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'videos/rf.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dai_roi(video_path,net,detection_conf=0.25,nms_overlap=0.2,iou_thresh=0.08,output='rf_roi.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
